{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388da812-8c59-4567-85c9-8590a0f17951",
   "metadata": {},
   "source": [
    "# 2-Layer Bayesian Neural Network with NumPy\n",
    "This notebook trains a 2 layer neural network on the [MNIST digit recognizer set](https://www.kaggle.com/competitions/digit-recognizer/data). The purpose of this notebook is to have a simple starting point to use building a Bayesian neural network using variaional inference. Also, this neural network is built without using PyTorch, instead all functions are defined using NumPy so that the mathematical steps are fully shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeef4dd-3b96-4185-9669-dbcdf3cfc91b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7e6f17-f1ea-4473-b668-f66c15a92404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2383bb1d-2fbe-47a1-9721-b2dbfade1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data (MNIST)\n",
    "data = pd.read_csv('/Users/kelseychen/Downloads/digit-recognizer/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cc5d2-0878-40cc-a710-3dc63446bc10",
   "metadata": {},
   "source": [
    "The data contains images of hand drawn digits between 0-9 that are 28x28 pixels each with a pixel value btween 0-255 to denote the shade of the pixel. Here 0 represents white and 255 represents black. \n",
    "\n",
    "The neural network will have two layer architecure where the input layer has 784 neurons. The hiden layer uses ReLU activation and the final layer uses softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db22816c-09f9-4206-a7d2-b1ffb6aa6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to np array\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "# shuffle the data before splitting into training and test sets\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# split the data\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255. # normalize \n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255. # normalize \n",
    "\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497210b-1767-4159-ad40-bb2a2e5c8400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 1, ..., 2, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4d981-3655-4696-a93c-d6bb43f24599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Activation and Utility Functions\n",
    "\n",
    "ReLU activation in the hidden layers of the network. The derivative of ReLu is used during backpropogation. \n",
    "\n",
    "Softmax is used to convert a vector of real numbers to a probability distribution and it used in the output layer of the network\n",
    "\n",
    "One-hot is used to convert class labels to encoded vectors. This function s used to compute the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0214f88c-0ca2-4e9b-bef5-5fb117a5e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    Z -= np.max(Z, axis=0, keepdims=True)\n",
    "    expZ = np.exp(Z)\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "def one_hot(Y, num_classes=10):\n",
    "    return np.eye(num_classes)[Y].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569dcfe5-b6c8-4b48-a586-50dc4c998e9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Weights as Distributions\n",
    "\n",
    "Below we initialize the variational parameters for each weight and bias in the neural network. For a Bayesian neural network, we assume each weight is a distribution instead of having the traditional fixed weights. In this case, we assume a normal distribution over the weights. To stabilize the calculations we will work with $$\\mu$$ (the mean of the distribution) and $$log(\\sigma)$$ (the log of the standard deviation).\n",
    "\n",
    "In `sample weights` we sample the weights from the distribution by using the reparameterization trick. They are then used in the forward pass to calculate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1550ef-9132-474c-8c8d-6b96c6e21879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a Normal distribution \n",
    "def init_variational_params(input_dim, hidden_dim, output_dim):\n",
    "    params = {}\n",
    "    for name, shape in {\n",
    "        'W1': (hidden_dim, input_dim),\n",
    "        'b1': (hidden_dim, 1),\n",
    "        'W2': (output_dim, hidden_dim),\n",
    "        'b2': (output_dim, 1)\n",
    "    }.items():\n",
    "        # initialize the mean of each weight\n",
    "        if 'W' in name:\n",
    "            fan_in = shape[1]\n",
    "            params[name + '_mu'] = np.random.randn(*shape) * np.sqrt(2. / fan_in)\n",
    "        else:\n",
    "\n",
    "            params[name + '_mu'] = np.zeros(shape)\n",
    "\n",
    "        # initialize the log standard deviation \n",
    "        params[name + '_log_std'] = np.full(shape, -3.0)  # small std at first\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca24709b-5987-4adf-8ad2-74877e49ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weights(params):\n",
    "    weights, eps_cache = {}, {}\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        mu = params[key + '_mu']\n",
    "\n",
    "        # convert to std\n",
    "        std = np.exp(params[key + '_log_std'])\n",
    "\n",
    "        # sample epsilon from N(0,1)\n",
    "        eps = np.random.randn(*mu.shape)\n",
    "\n",
    "        # sample w ~ N(mu, std^2)\n",
    "        weights[key] = mu + std * eps\n",
    "        eps_cache[key] = eps\n",
    "    return weights, eps_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664467cd-8f37-4778-8c4d-00479cad5dbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Forward and Backwards Pass \n",
    "\n",
    "In the forward pass we want to compute the pre-activation input to the hidden layer that is \n",
    "$$\n",
    "Z^{(1)} = W^{(1)}X+b^{(1)}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "W^{(1)} \\in \\mathbb R^{h\\times d} \\quad \\text{ are the weights} \n",
    "$$\n",
    " \n",
    "$$\n",
    "X \\in \\mathbb  R^{d\\times m} \\quad \\text{ are the input features} \n",
    "$$\n",
    " \n",
    "$$\n",
    "b^{(1)}) \\in  R^{h\\times 1} \\quad \\text{ is the bias vector}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a946789-bd63-415b-bd8f-670e785f2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights):\n",
    "    Z1 = weights['W1'] @ X + weights['b1']\n",
    "\n",
    "    # apply ReLU activation to introduce non-linearity \n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # copute the pre-activation input\n",
    "    Z2 = weights['W2'] @ A1 + weights['b2']\n",
    "    \n",
    "    # apply softmax activation to produce probabilities\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    # save values to use in backprop\n",
    "    cache = {'X': X, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ad02a-9bc4-48f5-a299-13204dbe8f4b",
   "metadata": {},
   "source": [
    "The function `backward` computes  \n",
    "$$\n",
    "\\nabla_{\\mu, \\log \\sigma} \\mathrm{ELBO}\n",
    "= \\nabla_{\\mu, \\log \\sigma}\n",
    "\\left[\\, -\\log p(y \\mid x, w) \\;+\\; \\mathrm{KL}(q(w)\\,\\|\\,p(w)) \\right],\n",
    "$$  \n",
    "where the first term corresponds to the negative log-likelihood and the second term is the KL divergence between the variational posterior $q(w)$ and a standard normal prior $ p(w) $.\n",
    "1. Compute the gradient of the negative log-likelihood\n",
    "\n",
    "Let  \n",
    "$$\n",
    "\\hat{Y} = A^{(2)} \\in \\mathbb{R}^{k \\times m}, \\quad \\text{(predicted probabilities)}\n",
    "$$\n",
    "$$\n",
    "Y \\in \\mathbb{R}^{k \\times m}, \\quad \\text{(one-hot encoded true labels)}\n",
    "$$\n",
    "\n",
    "Then the gradient of the loss with respect to the output logits is:\n",
    "$$\n",
    "dZ^{(2)} = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "2. Compute the gradients of the output weights and biases\n",
    "\n",
    "Let $ m $ be the number of examples in the batch:\n",
    "$$\n",
    "\\nabla_{W^{(2)}_\\mu} = \\frac{1}{m} dZ^{(2)} (A^{(1)})^T\n",
    "$$\n",
    "$$\n",
    "\\nabla_{b^{(2)}_\\mu} = \\frac{1}{m} \\sum dZ^{(2)}\n",
    "$$\n",
    "\n",
    "3. Backpropagate to the hidden layer\n",
    "\n",
    "Propagate the error backward:\n",
    "$$\n",
    "dA^{(1)} = (W^{(2)})^T dZ^{(2)}\n",
    "$$\n",
    "$$\n",
    "dZ^{(1)} = dA^{(1)} \\circ \\text{ReLU}'(Z^{(1)})\n",
    "$$\n",
    "\n",
    "Then compute the gradients:\n",
    "$$\n",
    "\\nabla_{W^{(1)}_\\mu} = \\frac{1}{m} dZ^{(1)} X^T\n",
    "$$\n",
    "$$\n",
    "\\nabla_{b^{(1)}_\\mu} = \\frac{1}{m} \\sum dZ^{(1)}\n",
    "$$\n",
    "\n",
    "4. Add the KL divergence gradients\n",
    "\n",
    "Assume the variational posterior:\n",
    "$$\n",
    "q(w) = \\mathcal{N}(\\mu, \\sigma^2), \\quad \\text{with prior } p(w) = \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "Then the gradients of the KL divergence term are:\n",
    "\n",
    "- With respect to the mean $ \\mu $:\n",
    "$$\n",
    "\\nabla_{\\mu} \\mathrm{KL} = \\frac{\\mu}{m}\n",
    "$$\n",
    "\n",
    "- With respect to the log standard deviation$\\log \\sigma $, using $ \\sigma = e^{\\\\log \\sigma} $:\n",
    "$$\n",
    "\\nabla_{\\\\log \\sigma} \\mathrm{KL} = \\frac{1}{m} (\\sigma^2 - 1)\n",
    "$$\n",
    "\n",
    "5. Include the reparameterization term\n",
    "\n",
    "Since weights are sampled as:\n",
    "$$\n",
    "w = \\mu + \\sigma \\cdot \\epsilon,\n",
    "$$\n",
    "\n",
    "the derivative of the likelihood with respect to $\\log \\sigma$ includes:\n",
    "$$\n",
    "\\nabla_{\\log \\sigma} \\mathcal{L}_{\\text{likelihood}} = \\sigma \\cdot \\epsilon \\cdot \\nabla_{\\mu} \\mathcal{L}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "###  Final gradients used to update parameters\n",
    "\n",
    "For each weight or bias:\n",
    "$$\n",
    "\\nabla_{\\mu} \\mathcal{L}_{\\text{ELBO}} = \\nabla_{\\mu} \\mathcal{L}_{\\text{likelihood}} + \\frac{\\mu}{m}\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\log \\sigma} \\mathcal{L}_{\\text{ELBO}} = \\sigma \\cdot \\epsilon \\cdot \\nabla_{\\mu} \\mathcal{L}_{\\text{likelihood}} + \\frac{\\sigma^2 - 1}{m}\n",
    "$$\n",
    "\n",
    "These gradients are returned and used to update the variational parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfd99ef-0cb0-4665-96b3-1abb77a7cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient computation using KL scaling\n",
    "def backward(Y, cache, weights, eps_cache, params, m, kl_weight):\n",
    "    grads = {}\n",
    "\n",
    "    Y_hat = cache['A2']\n",
    "\n",
    "    # compute the derivative of softmax\n",
    "    dZ2 = Y_hat - Y  \n",
    "\n",
    "    # backprop from output to hidden\n",
    "    grads['W2_mu'] = (1/m) * dZ2 @ cache['A1'].T\n",
    "    grads['b2_mu'] = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # backprop through ReLu and hidden layer \n",
    "    dA1 = weights['W2'].T @ dZ2\n",
    "    dZ1 = dA1 * relu_deriv(cache['Z1'])\n",
    "\n",
    "    grads['W1_mu'] = (1/m) * dZ1 @ cache['X'].T\n",
    "    grads['b1_mu'] = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    # store likelihood gradients separately before adding KL terms\n",
    "    likelihood_grads = {}\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        likelihood_grads[key+'_mu'] = grads[key+'_mu'].copy()\n",
    "\n",
    "    # Add KL gradients\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        mu = params[key + '_mu']\n",
    "        log_std = params[key + '_log_std']\n",
    "        std = np.exp(log_std)\n",
    "        eps = eps_cache[key]\n",
    "\n",
    "        # KL gradient with respect to mu: scale by kl_weight\n",
    "        grads[key + '_mu'] = likelihood_grads[key+'_mu'] + kl_weight * (mu / m)\n",
    "\n",
    "        # KL gradient with respect to log_std: using likelihood grad for mu\n",
    "        grads[key + '_log_std'] = std * eps * likelihood_grads[key+'_mu'] + kl_weight * ((std**2 - 1) / m)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cac21-93dc-4e31-bfea-106ab41f8c39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compute Loss and Train Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac8fba-bad5-469d-a50a-2a1feb37a759",
   "metadata": {},
   "source": [
    "The function `compute_elbo` calculates the Evidence Lower Bound (ELBO), which is the total loss function optimized in variational inference. It consists of two terms: the negative log-likelihood (data fit) and the KL divergence (regularization).\n",
    "\n",
    "The ELBO is given by:\n",
    "    $$ELBO = -log p(y | x, w) + KL(q(w) || p(w))$$\n",
    "\n",
    "1. Compute the negative log-likelihood\n",
    "\n",
    "Assume a softmax output and use the cross-entropy loss:\n",
    "   $$ NLL = -(1/m) * \\sum_{i=1}^m \\sum_{c=1}^k Y_{c,i} * log(Y_hat_{c,i})$$\n",
    "\n",
    "where:\n",
    "- `Y` is the one-hot encoded true label matrix of shape (k × m)\n",
    "- `Y_hat` is the predicted softmax output of shape (k × m)\n",
    "- `m` is the number of examples in the batch\n",
    "\n",
    "2. Compute the KL divergence\n",
    "\n",
    "For each variational parameter q(w) = N(μ, σ²), assuming a standard normal prior p(w) = N(0, 1), the KL divergence is:\n",
    "    $$KL = (1/2) * \\sum(μ² + σ² - 1 - 2 * log(σ))$$\n",
    "\n",
    "Since `σ = exp(log_std)`, we compute this as:\n",
    "\n",
    "```python\n",
    "kl += 0.5 * np.sum(mu**2 + std**2 - 1 - 2 * log_std)\n",
    "```\n",
    "\n",
    "3. Return the total ELBO\n",
    "\n",
    "The final loss is:\n",
    "    $$ELBO = NLL + (KL / m)$$\n",
    "\n",
    "This scaling ensures that both terms are on similar scales, especially when using minibatches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a1dedb-5ced-49f3-bc4f-4f46c4fd64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo(Y, Y_hat, params, kl_weight=1.0):\n",
    "    m = Y.shape[1]\n",
    "    log_likelihood = -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
    "    kl = 0\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        mu = params[key + '_mu']\n",
    "        log_std = params[key + '_log_std']\n",
    "        std = np.exp(log_std)\n",
    "        kl += 0.5 * np.sum(mu**2 + std**2 - 1 - 2 * log_std)\n",
    "    elbo = log_likelihood + kl_weight * (kl / m)\n",
    "    return elbo, log_likelihood, kl / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac7d77-5c9f-496f-829e-50fbda405e0c",
   "metadata": {},
   "source": [
    "The function `train` performs training of the Bayesian neural network by minimizing the ELBO loss using stochastic gradient descent with mini-batches, learning rate decay, and a KL weight ramp-up schedule.\n",
    "\n",
    "1. Learning rate and KL weight schedule\n",
    "\n",
    "- The learning rate decays exponentially each epoch:\n",
    "     $$lr_{epoch} = lr * (lr_{decay} ** epoch)$$\n",
    "\n",
    "- The KL weight increases gradually using a logistic ramp-up function centered near the end of training:\n",
    "      $$kl_{weight} = 1 / (1 + exp(-(epoch - ramp_{center}) / ramp_{scale}))$$\n",
    "\n",
    "\n",
    "2. Shuffle and split into mini-batches\n",
    "\n",
    "The data is randomly shuffled and split into mini-batches of size `batch_size`. For each batch:\n",
    "\n",
    "- Extract batch inputs `X_batch` and labels `Y_batch`\n",
    "- Convert `Y_batch` into one-hot encoded format `Y_batch_oh`\n",
    "\n",
    "\n",
    "3. Forward and backward pass\n",
    "\n",
    "For each mini-batch:\n",
    "\n",
    "- Sample weights from the variational distribution using reparameterization\n",
    "- Compute predictions using `forward`\n",
    "- Compute gradients using `backward` with respect to both `mu` and `log_std`\n",
    "\n",
    "4. Parameter updates\n",
    "\n",
    "The parameters are updated using stochastic gradient descent:\n",
    "\n",
    "- Gradients for `mu` are updated with the current learning rate\n",
    "- Gradients for `log_std` are updated with a 10× larger learning rate to accelerate uncertainty tuning:\n",
    "\n",
    "\n",
    "5. ELBO loss tracking\n",
    "\n",
    "After each batch:\n",
    "- Compute the ELBO, negative log-likelihood, and KL term using `compute_elbo`\n",
    "- Track the average ELBO loss over the entire epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a844b8d-b19f-423f-b640-a2471dd58ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, params, epochs=50, lr=0.05, batch_size=64, lr_decay=0.98):\n",
    "    m_total = Y.shape[0]\n",
    "    # Set a slower ramp-up: ramp center at 80% of epochs, ramp scale at 10% of epochs\n",
    "    ramp_center = epochs * 0.8   # For 50 epochs, ramp_center=40\n",
    "    ramp_scale = epochs * 0.1    # For 50 epochs, ramp_scale=5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Decay the learning rate gradually\n",
    "        lr_epoch = lr * (lr_decay ** epoch)\n",
    "        # Compute KL weight using a logistic ramp-up function\n",
    "        kl_weight = 1.0 / (1.0 + np.exp(-(epoch - ramp_center) / ramp_scale))\n",
    "        \n",
    "        # Shuffle the training data\n",
    "        permutation = np.random.permutation(m_total)\n",
    "        X_shuffled = X[:, permutation]\n",
    "        Y_shuffled = Y[permutation]\n",
    "        epoch_loss = 0\n",
    "        num_batches = int(np.ceil(m_total / batch_size))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i+1) * batch_size, m_total)\n",
    "            X_batch = X_shuffled[:, start:end]\n",
    "            Y_batch = Y_shuffled[start:end]\n",
    "            Y_batch_oh = one_hot(Y_batch)\n",
    "            m_batch = Y_batch.shape[0]\n",
    "            \n",
    "            weights, eps_cache = sample_weights(params)\n",
    "            Y_hat, cache = forward(X_batch, weights)\n",
    "            grads = backward(Y_batch_oh, cache, weights, eps_cache, params, m_batch, kl_weight)\n",
    "            \n",
    "            # Update parameters (using a boosted learning rate for log_std parameters)\n",
    "            for key in grads:\n",
    "                if 'log_std' in key:\n",
    "                    params[key] -= (lr_epoch * 10) * grads[key]\n",
    "                else:\n",
    "                    params[key] -= lr_epoch * grads[key]\n",
    "            \n",
    "            elbo, ll, kl_term = compute_elbo(Y_batch_oh, Y_hat, params, kl_weight)\n",
    "            epoch_loss += elbo\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}, lr: {lr_epoch:.5f}, KL weight: {kl_weight:.3f}, Avg Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc91f44-ecac-4946-9576-ae0b63fd81ee",
   "metadata": {},
   "source": [
    "The function `predict` performs prediction using the Bayesian neural network by averaging over multiple samples from the variational distribution. This allows the model to capture predictive uncertainty.\n",
    "\n",
    "1. Monte Carlo sampling of weights\n",
    "Instead of making a prediction using a single set of weights, the function samples weights multiple times from the variational posterior:\n",
    "\n",
    "```python\n",
    "weights, _ = sample_weights(params)\n",
    "```\n",
    "\n",
    "For each sample, it performs a forward pass:\n",
    "\n",
    "```python\n",
    "probs, _ = forward(X, weights)\n",
    "```\n",
    "\n",
    "This gives a probability distribution over the output classes for each input.\n",
    "\n",
    "2. Average the predictions\n",
    "\n",
    "Each prediction is added to a running total:\n",
    "\n",
    "```python\n",
    "probs_sum += probs\n",
    "```\n",
    "\n",
    "After `num_samples` forward passes, we compute the average predicted probabilities:\n",
    "\n",
    "```python\n",
    "avg_probs = probs_sum / num_samples\n",
    "```\n",
    "\n",
    "This approximates the predictive distribution:\n",
    "    $$ p(y | x) ≈ (1/S) \\sum_{s=1}^S p(y | x, w_s) $$\n",
    "\n",
    "where each `w_s` is a sample from the variational posterior.\n",
    "\n",
    "\n",
    "3. Final prediction\n",
    "\n",
    "The final class prediction is taken as the argmax of the averaged probabilities:\n",
    "\n",
    "```python\n",
    "return np.argmax(avg_probs, axis=0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632bf9a8-e575-44cd-a758-e5037f73aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, params, num_samples=50):\n",
    "    m = X.shape[1]\n",
    "    probs_sum = np.zeros((10, m))\n",
    "    for _ in range(num_samples):\n",
    "        weights, _ = sample_weights(params)\n",
    "        probs, _ = forward(X, weights)\n",
    "        probs_sum += probs\n",
    "    avg_probs = probs_sum / num_samples\n",
    "    return np.argmax(avg_probs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669eaf1-485c-4ca2-bc86-2877a81b5682",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f7ac9-315c-4c75-b8ad-e9872a4d7e7f",
   "metadata": {},
   "source": [
    "This block runs the full training and evaluation pipeline.\n",
    "\n",
    "1. Initialize variational parameters\n",
    "\n",
    "```python\n",
    "params = init_variational_params(784, 128, 10)\n",
    "```\n",
    "\n",
    "Initializes the means (`mu`) and log standard deviations (`log_std`) for all weights and biases in the network.\n",
    "\n",
    "We inspect the initial uncertainty (log std) of the first layer:\n",
    "\n",
    "```python\n",
    "print(\"W1_log_std before training (mean):\", np.mean(params['W1_log_std']))\n",
    "```\n",
    "\n",
    "2. Train the model\n",
    "\n",
    "```python\n",
    "train(X_train, Y_train, params, epochs=10, lr=0.05)\n",
    "```\n",
    "\n",
    "Trains the Bayesian neural network for 10 epochs using the training set. The function minimizes the ELBO loss, combining both likelihood and KL divergence terms.\n",
    "\n",
    "3. Predict on test set using Monte Carlo averaging\n",
    "\n",
    "```python\n",
    "Y_pred = predict(X_test, params, num_samples=30)\n",
    "```\n",
    "\n",
    "Performs prediction by averaging the output of 30 different forward passes with sampled weights. This approximates the posterior predictive distribution.\n",
    "\n",
    "\n",
    "4. Compute test accuracy\n",
    "\n",
    "```python\n",
    "acc = np.mean(Y_pred == Y_test)\n",
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "```\n",
    "Compares predicted class labels with true labels to compute classification accuracy.\n",
    "\n",
    "5. Inspect learned uncertainty\n",
    "\n",
    "```python\n",
    "print(\"W1_log_std after training (mean):\", np.mean(params['W1_log_std']))\n",
    "```\n",
    "\n",
    "Prints the average log standard deviation of the first layer's weights after training. This helps verify that the model has learned to adjust its uncertainty.\n",
    "\n",
    "If `W1_log_std` increased or decreased significantly, it reflects how the model adapted its confidence in the weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0902f89d-c8dd-405d-8579-0e89cbcae027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_log_std before training (mean): -3.0\n",
      "Epoch 1, lr: 0.05000, KL weight: 0.000, Avg Loss: 1.9772\n",
      "Epoch 2, lr: 0.04900, KL weight: 0.001, Avg Loss: 3.9842\n",
      "Epoch 3, lr: 0.04802, KL weight: 0.002, Avg Loss: 10.1142\n",
      "Epoch 4, lr: 0.04706, KL weight: 0.007, Avg Loss: 26.5890\n",
      "Epoch 5, lr: 0.04612, KL weight: 0.018, Avg Loss: 69.3617\n",
      "Epoch 6, lr: 0.04520, KL weight: 0.047, Avg Loss: 171.3388\n",
      "Epoch 7, lr: 0.04429, KL weight: 0.119, Avg Loss: 360.7525\n",
      "Epoch 8, lr: 0.04341, KL weight: 0.269, Avg Loss: 471.4871\n",
      "Epoch 9, lr: 0.04254, KL weight: 0.500, Avg Loss: 137.5842\n",
      "Epoch 10, lr: 0.04169, KL weight: 0.731, Avg Loss: 21.2316\n",
      "Test Accuracy: 76.40%\n",
      "W1_log_std after training (mean): -0.01244816787320679\n"
     ]
    }
   ],
   "source": [
    "params = init_variational_params(784, 128, 10)\n",
    "print(\"W1_log_std before training (mean):\", np.mean(params['W1_log_std']))\n",
    "\n",
    "train(X_train, Y_train, params, epochs=10, lr=0.05)\n",
    "\n",
    "Y_pred = predict(X_test, params, num_samples=30)\n",
    "acc = np.mean(Y_pred == Y_test)\n",
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "print(\"W1_log_std after training (mean):\", np.mean(params['W1_log_std']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a1b3b-6948-4fb3-a1ac-f3c58f105db5",
   "metadata": {},
   "source": [
    "## View Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dcd64a0-e933-4088-bdf4-fad7267d6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def make_predictions(X, params):\n",
    "    # Sample one set of weights (deterministic forward pass)\n",
    "    weights, _ = sample_weights(params)\n",
    "    A2, _ = forward(X, weights)\n",
    "    predictions = np.argmax(A2, axis=0)\n",
    "    return predictions, weights\n",
    "\n",
    "def test_prediction(index, params, num_samples=30):\n",
    "    # Extract and reshape image\n",
    "    current_image = X_train[:, index, None]\n",
    "    label = Y_train[index]\n",
    "\n",
    "    # Run multiple forward passes to compute predictive distribution\n",
    "    probs_sum = np.zeros((10, 1))\n",
    "    for _ in range(num_samples):\n",
    "        weights, _ = sample_weights(params)\n",
    "        A2, _ = forward(current_image, weights)\n",
    "        probs_sum += A2\n",
    "\n",
    "    avg_probs = probs_sum / num_samples\n",
    "    entropy = scipy.stats.entropy(avg_probs[:, 0], base=2)\n",
    "\n",
    "    # Final predicted class\n",
    "    prediction = np.argmax(avg_probs, axis=0)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Prediction:\", prediction[0])\n",
    "    print(\"Label:     \", label)\n",
    "    print(\"Entropy:   {:.4f}\".format(entropy))\n",
    "\n",
    "    # Display the image\n",
    "    img = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccfa4e28-5eae-40d7-b771-61cf71302f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n",
      "Label:      7\n",
      "Entropy:   0.3549\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 6\n",
      "Label:      6\n",
      "Entropy:   2.6932\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Label:      1\n",
      "Entropy:   0.3977\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n",
      "Label:      0\n",
      "Entropy:   1.9246\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(0, params)\n",
    "test_prediction(1, params)\n",
    "test_prediction(2, params)\n",
    "test_prediction(3, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be897a3-8109-44e4-8aa4-36359362c00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
